{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea01ff22-667c-4594-89de-7804e0b971af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import tensorflow\n",
    "import os\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55d0c19a-7b6e-439e-bcb6-8d2214626a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = r\"C:\\Users\\mohit\\Downloads\\mixed\\mixed\"\n",
    "image_files = []\n",
    "labels = []\n",
    "directory = {'none': 0, 'red': 1, 'yellow': 2, 'green': 3}\n",
    "\n",
    "for class_folder in os.listdir(data_dir):\n",
    "    class_folder_path = os.path.join(data_dir, class_folder)\n",
    "    if os.path.isdir(class_folder_path) and class_folder in directory:\n",
    "        class_label = directory[class_folder]\n",
    "\n",
    "        class_image_files = [file for file in os.listdir(class_folder_path) if file.endswith('.jpg')]\n",
    "\n",
    "        labels.extend([class_label] * len(class_image_files))\n",
    "\n",
    "        image_files.extend([os.path.join(class_folder_path, file) for file in class_image_files])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d4dc6-31f4-4c02-9c2b-355e78a523ea",
   "metadata": {},
   "source": [
    "Spliting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e34e64e1-d131-4b09-86b2-cb51f0db8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train , y_test = train_test_split(image_files,labels,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6dd569f9-3d5b-49f2-aa43-12ae455ce9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9d2dc7d-04f2-48a6-9a8c-2b79cd4e0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=[i for i in labels if i==0]\n",
    "out1=[i for i in labels if i==1]\n",
    "out2=[i for i in labels if i==2]\n",
    "out3=[i for i in labels if i==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d729b451-a030-4241-b104-994f0c47f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593 592 50 240\n"
     ]
    }
   ],
   "source": [
    "print(len(out), len(out1) , len(out2) , len(out3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da55f1-5707-4c27-8da8-28706f032aa8",
   "metadata": {},
   "source": [
    "model will genrelize better without class imbalance. preprocessing the image , Applying RandomOverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3151e88c-ce2a-41cc-b62d-3eb6bc932d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "\n",
    "def preprocess_image(file_path, label):\n",
    "    img = load_img(file_path, target_size=(img_width, img_height))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array / 255.0 \n",
    "    return img_array, label\n",
    "\n",
    "x_train_processed = []\n",
    "for img_path, label in zip(x_train, y_train):\n",
    "    img_array, label = preprocess_image(img_path, label)\n",
    "    x_train_processed.append(img_array)\n",
    "\n",
    "x_train_processed = np.array(x_train_processed)\n",
    "y_train_categorical = to_categorical(y_train) \n",
    "\n",
    "\n",
    "x_test_processed = []\n",
    "for img_path, label in zip(x_test, y_test):\n",
    "    img_array, label = preprocess_image(img_path, label)\n",
    "    x_test_processed.append(img_array)\n",
    "\n",
    "x_test_processed = np.array(x_test_processed)\n",
    "y_test_categorical = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "510592e9-5b8b-4745-a6c4-85b1c3a2dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flattened = x_train_processed.reshape(x_train_processed.shape[0], -1)\n",
    "ros=RandomOverSampler(random_state=42)\n",
    "x_resampled, y_resampled = ros.fit_resample(x_train_flattened, y_train_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7d284fa-320d-4a34-be6a-bb40f524c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = x_train_processed.shape[1:]\n",
    "x_resampled_original_shape = x_resampled.reshape(-1, *original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a39189f-c76b-4eba-b4c8-e15eb3e49476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5072, 224, 224, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_resampled_original_shape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd2f994d-43b4-425a-891d-9d06d0588244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5072"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d848ff8a-11c2-48bf-97c7-3ac0dcd39cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 224, 224, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f8aa7-242c-4123-8d63-e12f236afc12",
   "metadata": {},
   "source": [
    "final images for training  x_resampled_original_shape and  y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc13f138-8747-41ba-bebf-d2afb5c73864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_resampled_original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31402042-6450-4df9-8892-63dca1b6185f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18fbf16b-53c7-4697-a80a-fa33251a01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    rescale=None)\n",
    "\n",
    "train_generator = datagen.flow(x_resampled_original_shape, y_resampled, batch_size=100, shuffle=True)\n",
    "val_generator = datagen.flow(x_test_processed, y_test_categorical, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c760f2d9-f2d8-4a54-87d2-461c994c1c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c595e28-1308-416a-ba71-db494ebe7c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 112, 112, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 200704)            0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200704)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 528)               105972240 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 2116      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105993748 (404.33 MB)\n",
      "Trainable params: 105993748 (404.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 2.8114 - accuracy: 0.3829\n",
      "Epoch 1: val_loss improved from inf to 1.27416, saving model to saved_models\\model_epoch_01.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_01.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_01.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 132s 3s/step - loss: 2.8114 - accuracy: 0.3829 - val_loss: 1.2742 - val_accuracy: 0.2909\n",
      "Epoch 2/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.1652 - accuracy: 0.4852\n",
      "Epoch 2: val_loss improved from 1.27416 to 1.05022, saving model to saved_models\\model_epoch_02.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_02.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_02.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 129s 3s/step - loss: 1.1652 - accuracy: 0.4852 - val_loss: 1.0502 - val_accuracy: 0.5152\n",
      "Epoch 3/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.0316 - accuracy: 0.5657\n",
      "Epoch 3: val_loss improved from 1.05022 to 0.90116, saving model to saved_models\\model_epoch_03.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_03.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_03.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 130s 3s/step - loss: 1.0316 - accuracy: 0.5657 - val_loss: 0.9012 - val_accuracy: 0.5677\n",
      "Epoch 4/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.8592 - accuracy: 0.6443\n",
      "Epoch 4: val_loss improved from 0.90116 to 0.78633, saving model to saved_models\\model_epoch_04.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_04.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_04.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 125s 2s/step - loss: 0.8592 - accuracy: 0.6443 - val_loss: 0.7863 - val_accuracy: 0.6323\n",
      "Epoch 5/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.7315 - accuracy: 0.7029\n",
      "Epoch 5: val_loss did not improve from 0.78633\n",
      "51/51 [==============================] - 127s 2s/step - loss: 0.7315 - accuracy: 0.7029 - val_loss: 1.0901 - val_accuracy: 0.4505\n",
      "Epoch 6/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.6576 - accuracy: 0.7382\n",
      "Epoch 6: val_loss improved from 0.78633 to 0.65669, saving model to saved_models\\model_epoch_06.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_06.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_06.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 137s 3s/step - loss: 0.6576 - accuracy: 0.7382 - val_loss: 0.6567 - val_accuracy: 0.7374\n",
      "Epoch 7/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.7658\n",
      "Epoch 7: val_loss improved from 0.65669 to 0.59837, saving model to saved_models\\model_epoch_07.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_07.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_07.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 133s 3s/step - loss: 0.5937 - accuracy: 0.7658 - val_loss: 0.5984 - val_accuracy: 0.7859\n",
      "Epoch 8/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.5182 - accuracy: 0.8046\n",
      "Epoch 8: val_loss improved from 0.59837 to 0.52642, saving model to saved_models\\model_epoch_08.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_08.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_08.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 130s 3s/step - loss: 0.5182 - accuracy: 0.8046 - val_loss: 0.5264 - val_accuracy: 0.7980\n",
      "Epoch 9/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.4699 - accuracy: 0.8200\n",
      "Epoch 9: val_loss improved from 0.52642 to 0.40344, saving model to saved_models\\model_epoch_09.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_09.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_09.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 132s 3s/step - loss: 0.4699 - accuracy: 0.8200 - val_loss: 0.4034 - val_accuracy: 0.8485\n",
      "Epoch 10/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.5244 - accuracy: 0.8233\n",
      "Epoch 10: val_loss improved from 0.40344 to 0.38826, saving model to saved_models\\model_epoch_10.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_10.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_10.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 125s 2s/step - loss: 0.5244 - accuracy: 0.8233 - val_loss: 0.3883 - val_accuracy: 0.8646\n",
      "Epoch 11/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.4812 - accuracy: 0.8273\n",
      "Epoch 11: val_loss did not improve from 0.38826\n",
      "51/51 [==============================] - 125s 2s/step - loss: 0.4812 - accuracy: 0.8273 - val_loss: 0.4571 - val_accuracy: 0.8222\n",
      "Epoch 12/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.8425\n",
      "Epoch 12: val_loss improved from 0.38826 to 0.33347, saving model to saved_models\\model_epoch_12.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_12.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_12.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 128s 3s/step - loss: 0.3985 - accuracy: 0.8425 - val_loss: 0.3335 - val_accuracy: 0.8747\n",
      "Epoch 13/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.8476\n",
      "Epoch 13: val_loss did not improve from 0.33347\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.4108 - accuracy: 0.8476 - val_loss: 0.3621 - val_accuracy: 0.8727\n",
      "Epoch 14/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8543\n",
      "Epoch 14: val_loss improved from 0.33347 to 0.31234, saving model to saved_models\\model_epoch_14.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_14.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_14.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 225s 4s/step - loss: 0.3676 - accuracy: 0.8543 - val_loss: 0.3123 - val_accuracy: 0.8747\n",
      "Epoch 15/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8584\n",
      "Epoch 15: val_loss did not improve from 0.31234\n",
      "51/51 [==============================] - 225s 4s/step - loss: 0.3548 - accuracy: 0.8584 - val_loss: 0.4649 - val_accuracy: 0.8141\n",
      "Epoch 16/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.8677\n",
      "Epoch 16: val_loss did not improve from 0.31234\n",
      "51/51 [==============================] - 225s 4s/step - loss: 0.3410 - accuracy: 0.8677 - val_loss: 0.3804 - val_accuracy: 0.8525\n",
      "Epoch 17/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.8671\n",
      "Epoch 17: val_loss improved from 0.31234 to 0.30652, saving model to saved_models\\model_epoch_17.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_17.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_17.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 230s 4s/step - loss: 0.3302 - accuracy: 0.8671 - val_loss: 0.3065 - val_accuracy: 0.8788\n",
      "Epoch 18/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.8746\n",
      "Epoch 18: val_loss did not improve from 0.30652\n",
      "51/51 [==============================] - 224s 4s/step - loss: 0.3104 - accuracy: 0.8746 - val_loss: 0.3421 - val_accuracy: 0.8646\n",
      "Epoch 19/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.8726\n",
      "Epoch 19: val_loss did not improve from 0.30652\n",
      "51/51 [==============================] - 222s 4s/step - loss: 0.3128 - accuracy: 0.8726 - val_loss: 0.3627 - val_accuracy: 0.8606\n",
      "Epoch 20/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.8785\n",
      "Epoch 20: val_loss did not improve from 0.30652\n",
      "51/51 [==============================] - 221s 4s/step - loss: 0.2935 - accuracy: 0.8785 - val_loss: 0.4234 - val_accuracy: 0.8364\n",
      "Epoch 21/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2924 - accuracy: 0.8839\n",
      "Epoch 21: val_loss did not improve from 0.30652\n",
      "51/51 [==============================] - 259s 5s/step - loss: 0.2924 - accuracy: 0.8839 - val_loss: 0.3594 - val_accuracy: 0.8848\n",
      "Epoch 22/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.8878\n",
      "Epoch 22: val_loss did not improve from 0.30652\n",
      "51/51 [==============================] - 277s 5s/step - loss: 0.2774 - accuracy: 0.8878 - val_loss: 0.3269 - val_accuracy: 0.8667\n",
      "Epoch 23/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.8878\n",
      "Epoch 23: val_loss improved from 0.30652 to 0.27023, saving model to saved_models\\model_epoch_23.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_23.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_23.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 282s 6s/step - loss: 0.2782 - accuracy: 0.8878 - val_loss: 0.2702 - val_accuracy: 0.8848\n",
      "Epoch 24/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2718 - accuracy: 0.8908\n",
      "Epoch 24: val_loss improved from 0.27023 to 0.26723, saving model to saved_models\\model_epoch_24.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_24.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_24.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 287s 6s/step - loss: 0.2718 - accuracy: 0.8908 - val_loss: 0.2672 - val_accuracy: 0.8747\n",
      "Epoch 25/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.8941\n",
      "Epoch 25: val_loss improved from 0.26723 to 0.23224, saving model to saved_models\\model_epoch_25.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_25.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_25.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 283s 6s/step - loss: 0.2661 - accuracy: 0.8941 - val_loss: 0.2322 - val_accuracy: 0.9071\n",
      "Epoch 26/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.8943\n",
      "Epoch 26: val_loss did not improve from 0.23224\n",
      "51/51 [==============================] - 280s 5s/step - loss: 0.2659 - accuracy: 0.8943 - val_loss: 0.2769 - val_accuracy: 0.8707\n",
      "Epoch 27/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.8933\n",
      "Epoch 27: val_loss did not improve from 0.23224\n",
      "51/51 [==============================] - 290s 6s/step - loss: 0.2558 - accuracy: 0.8933 - val_loss: 0.2723 - val_accuracy: 0.9010\n",
      "Epoch 28/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.9032\n",
      "Epoch 28: val_loss did not improve from 0.23224\n",
      "51/51 [==============================] - 288s 6s/step - loss: 0.2442 - accuracy: 0.9032 - val_loss: 0.3354 - val_accuracy: 0.8586\n",
      "Epoch 29/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.9020\n",
      "Epoch 29: val_loss did not improve from 0.23224\n",
      "51/51 [==============================] - 286s 6s/step - loss: 0.2591 - accuracy: 0.9020 - val_loss: 0.2908 - val_accuracy: 0.8828\n",
      "Epoch 30/30\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.9006\n",
      "Epoch 30: val_loss improved from 0.23224 to 0.23175, saving model to saved_models\\model_epoch_30.ckpt\n",
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_30.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\model_epoch_30.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 331s 6s/step - loss: 0.2538 - accuracy: 0.9006 - val_loss: 0.2317 - val_accuracy: 0.8949\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "        \n",
    "model.add(Conv2D(32,(3,3),padding='same',input_shape=(224,224,3),activation=tensorflow.nn.relu))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "        \n",
    "model.add(Conv2D(64,(3,3),padding='same',activation=tensorflow.nn.relu))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(rate=0.5))\n",
    "        \n",
    "model.add(Dense(528,activation=tensorflow.nn.relu))\n",
    "model.add(Dense(4,activation=tensorflow.nn.softmax))\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint_path = \"saved_models/\"\n",
    "checkpoint_file = checkpoint_path + \"model_epoch_{epoch:02d}.ckpt\"\n",
    "checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(filepath=checkpoint_file,\n",
    "                                                                 save_weights_only=False,\n",
    "                                                                 save_best_only=True,\n",
    "                                                                 monitor='val_loss',\n",
    "                                                                 verbose=1)\n",
    "        \n",
    "history= model.fit(train_generator,epochs=30,verbose=1,validation_data=val_generator , callbacks=[checkpoint_callback],steps_per_epoch= 51, validation_steps=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d98f6b-41f2-469b-8120-a36233afba20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
